{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import imageio\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initials = np.load('images/initials.npy')\n",
    "len(initials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red - player \\\n",
    "Green - forest \\\n",
    "Blue - stones \n",
    "\n",
    "Moves: left, right, up, down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial = initials[456]\n",
    "plt.imshow(np.moveaxis(initial, (0, 1, 2), (2, 1, 0)) * 255);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raic2020Python:\n",
    "    def __init__(self, state, centr=True, game_length=200):\n",
    "        self.initial_state = state.copy()\n",
    "        self.state = None\n",
    "        self.unit = None\n",
    "        #self.actions = [(-1, -1, -1), (0, -1, 0), (0, 1, 0), (0, 0, -1), (0, 0, 1), (1, -1, 0), (1, 1, 0), (1, 0, -1), (1, 0, 1)]\n",
    "        self.actions = [(0, -1, 0), (0, 1, 0), (0, 0, -1), (0, 0, 1)]\n",
    "        self.tick = None\n",
    "        self.done = None\n",
    "        self.game_length = game_length\n",
    "        self.centr = centr\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = self.initial_state.copy()\n",
    "        i = self.state[0].argmax()\n",
    "        self.unit = i // 32, i % 32\n",
    "        self.tick = 0\n",
    "        self.done = False\n",
    "        if self.centr:\n",
    "            return self.centralize(self.state)\n",
    "        else:\n",
    "            return self.centralize(self.state)\n",
    "    \n",
    "    def step(self, i):\n",
    "        kind, dx, dy = self.actions[i]\n",
    "        x, y = self.unit\n",
    "        reward = 0\n",
    "        if x + dx < 0 or x + dx >= 32 or y + dy < 0 or y + dy >= 32:\n",
    "            kind = -1\n",
    "            \n",
    "        if kind == 0 and self.state[:, x + dx, y + dy].max() == 0:\n",
    "            self.state[0, x + dx, y + dy] = 1\n",
    "            self.state[0, x, y] = 0\n",
    "            self.unit = x + dx, y + dy\n",
    "        elif kind == 0 and self.state[1, x + dx, y + dy] == 1:\n",
    "            self.state[1, x + dx, y + dy] = 0\n",
    "            reward = 1\n",
    "            \n",
    "        self.tick += 1\n",
    "        self.done = self.tick == self.game_length\n",
    "        if self.centr:\n",
    "            return self.centralize(self.state.copy()), reward, self.done, {}\n",
    "        else:\n",
    "            return self.state.copy(), reward, self.done, {}\n",
    "    \n",
    "    \n",
    "    def centralize(self, state):\n",
    "        mask = np.zeros((3, 32, 32), dtype=np.uint8)\n",
    "        mask[2, :, :] = 1\n",
    "        x, y = self.unit\n",
    "        dx = 15 - x\n",
    "        dy = 15 - y\n",
    "        mask[:, max(dx, 0):min(32+dx, 32), max(dy, 0):min(32+dy, 32)] = state[:, max(-dx, 0):min(32-dx, 32), max(-dy, 0):min(32-dy, 32)]\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(torch.tensor(states).to(torch.float))\n",
    "    output = output.detach().numpy()\n",
    "    \n",
    "    return np.exp(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def image(state, axis=2):\n",
    "#    layer = state['map'][:32, :32, 0]\n",
    "#    return np.stack((layer == 3,layer == 8,(layer != 3)*(layer != 0)*(layer != 8)), axis=axis).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=500, tempr=2):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "        action_probs_copy = action_probs.copy()\n",
    "        action_probs = action_probs ** tempr\n",
    "        action_probs = action_probs / sum(action_probs)\n",
    "        \n",
    "        # Sample action with given probabilities.\n",
    "        \n",
    "        a = np.random.choice(range(len(action_probs)), p=action_probs)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    G = []\n",
    "    for r in reversed(rewards):\n",
    "        if not G:\n",
    "            G.append(r)\n",
    "        else:\n",
    "            G.append(r + gamma * G[-1])\n",
    "    \n",
    "    return G[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-1):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * to_one_hot(actions, len(env.actions)), dim=1)\n",
    "   \n",
    "    # Compute loss here. Don't forget entropy regularization with `entropy_coef` \n",
    "    entropy = -entropy_coef * (probs * log_probs).sum()\n",
    "    \n",
    "    \n",
    "    loss = torch.mean(log_probs_for_actions * cumulative_returns) + entropy\n",
    "    #loss = -entropy\n",
    "    \n",
    "    # Gradient descent step\n",
    "    optimizer.zero_grad()\n",
    "    (-loss).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(states):\n",
    "    images = []\n",
    "    for progress, state in enumerate(states):\n",
    "        im = state.transpose().astype(np.uint8) * 255\n",
    "        bar = np.zeros((1, 32, 3), dtype=np.uint8)\n",
    "        bar[:, :int((progress+1) / len(states) * 32), :] = 255\n",
    "        im = np.concatenate((im, bar), axis=0)\n",
    "        im = np.repeat(np.repeat(im, 10, axis=0), 10, axis=1)\n",
    "        #im = im[::-1, :, :]\n",
    "        #mask = (255 - im.max(axis=2))\n",
    "        #im = np.stack((mask, mask, mask), 2) + im\n",
    "        images.append(im)\n",
    "    imageio.mimsave('images/gif.gif', images, 'GIF', fps=30)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 6 * 6)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 1e-3\n",
    "ENTROPY_COEF = 1e-3\n",
    "TEMPERATURE = 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session=8\n",
      "reward=0.39520958083832336\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"images/gif.gif\" width=\"300\" align=\"center\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "\n",
    "    rewards = []\n",
    "    for _ in range(20):\n",
    "        initial_picture = initials[np.random.randint(1000)]\n",
    "        env = Raic2020Python(initial_picture, centr=True, game_length=500)\n",
    "        states, actions, rewards = generate_session(env, t_max=500, tempr=TEMPERATURE)\n",
    "        reward = train_on_session(states, actions, rewards, gamma=GAMMA, entropy_coef=ENTROPY_COEF)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    create_gif(states)\n",
    "    clear_output()\n",
    "    print (f'session={i}')\n",
    "    print (f'reward={np.mean(rewards)}')\n",
    "    display(HTML('<img src=\"images/gif.gif\" width=\"300\" align=\"center\">'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
